<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>The Software Factory: A Practitioner&#39;s Guide</title>
  <style>

code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}

ul.task-list[class]{list-style: none;}
ul.task-list li input[type="checkbox"] {
font-size: inherit;
width: 0.8em;
margin: 0 0.8em 0.2em -1.6em;
vertical-align: middle;
}
.display.math{display: block; text-align: center; margin: 0.5rem auto;}

html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
{ counter-reset: source-line 0; }
pre.numberSource code > span
{ position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
{ content: counter(source-line);
position: relative; left: -1em; text-align: right; vertical-align: baseline;
border: none; display: inline-block;
-webkit-touch-callout: none; -webkit-user-select: none;
-khtml-user-select: none; -moz-user-select: none;
-ms-user-select: none; user-select: none;
padding: 0 4px; width: 4em;
color: #aaaaaa;
}
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa; padding-left: 4px; }
div.sourceCode
{ }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } 
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } 
code span.at { color: #7d9029; } 
code span.bn { color: #40a070; } 
code span.bu { color: #008000; } 
code span.cf { color: #007020; font-weight: bold; } 
code span.ch { color: #4070a0; } 
code span.cn { color: #880000; } 
code span.co { color: #60a0b0; font-style: italic; } 
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } 
code span.do { color: #ba2121; font-style: italic; } 
code span.dt { color: #902000; } 
code span.dv { color: #40a070; } 
code span.er { color: #ff0000; font-weight: bold; } 
code span.ex { } 
code span.fl { color: #40a070; } 
code span.fu { color: #06287e; } 
code span.im { color: #008000; font-weight: bold; } 
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } 
code span.kw { color: #007020; font-weight: bold; } 
code span.op { color: #666666; } 
code span.ot { color: #007020; } 
code span.pp { color: #bc7a00; } 
code span.sc { color: #4070a0; } 
code span.ss { color: #bb6688; } 
code span.st { color: #4070a0; } 
code span.va { color: #19177c; } 
code span.vs { color: #4070a0; } 
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } 
</style>
  <style type="text/css">@page {
size: letter;
margin: 1in;
}
body {
font-family: Charter, Georgia, serif;
font-size: 11pt;
line-height: 1.5;
color: #1a1a1a;
}
h1, h2, h3, h4, h5, h6 {
margin-top: 1.4em;
margin-bottom: 0.6em;
line-height: 1.25;
}
h1 { font-size: 1.6em; }
h2 { font-size: 1.3em; border-bottom: 1px solid #ccc; padding-bottom: 0.3em; }
h3 { font-size: 1.1em; }
a {
color: #0366d6;
text-decoration: none;
}
code {
font-family: "PT Mono", "Courier New", monospace;
font-size: 0.9em;
background-color: #f6f8fa;
padding: 0.15em 0.3em;
border-radius: 3px;
}
pre {
font-family: "PT Mono", "Courier New", monospace;
font-size: 0.85em;
background-color: #f6f8fa;
padding: 0.8em 1em;
border-radius: 5px;
overflow-x: auto;
line-height: 1.4;
}
pre code {
background-color: transparent;
padding: 0;
}
blockquote {
border-left: 4px solid #ddd;
margin-left: 0;
padding-left: 1em;
color: #555;
}
hr {
border: none;
border-top: 1px solid #ccc;
margin: 2em 0;
}
table {
border-collapse: collapse;
width: 100%;
margin: 1em 0;
}
th, td {
border: 1px solid #ddd;
padding: 0.5em 0.75em;
text-align: left;
}
th {
background-color: #f6f8fa;
}
ul, ol {
padding-left: 1.5em;
}
li {
margin-bottom: 0.25em;
}
</style>
</head>
<body>
<header id="title-block-header">
<h1 class="title">The Software Factory: A Practitioner&#39;s Guide</h1>
</header>
<h1 id="the-software-factory-a-practitioners-guide-to-specification-driven-development-for-enterprise-services">The
Software Factory: A Practitioner&#39;s Guide to Specification-Driven
Development for Enterprise Services</h1>
<p><strong>A reference for implementing factory-pattern development
within an enterprise SOA.</strong></p>
<p><strong>Version 1</strong>: <a href="https://thewoolleyweb.com/software-factory-practitioners-guide-v01.pdf">PDF</a>
- <a href="https://github.com/thewoolleyman/software-factory-practitioners-guide/blob/main/software-factory-practitioners-guide-v01.md">Markdown</a>
- <a href="https://thewoolleyweb.com/software-factory-practitioners-guide-v01.html">HTML</a></p>
<p><a href="https://thewoolleyweb.com/software-factory-practitioners-guide"><strong>Latest
Version</strong></a> - <a href="https://thewoolleyweb.com/software-factory-practitioners-guide-versions"><strong>All
Versions</strong></a></p>
<p><strong>Chad Woolley, <a href="mailto:thewoolleyman@gmail.com">thewoolleyman@gmail.com</a>,
February 2026</strong></p>
<hr />
<h2 id="table-of-contents">Table of Contents</h2>
<ul>
<li><a href="#executive-summary">Executive Summary</a></li>
<li><a href="#audience-and-content">Audience and Content</a></li>
<li><a href="#1-overview-and-scope">1. Overview and Scope</a>
<ul>
<li><a href="#reality-check-this-is-an-aspirational-guide">Reality
Check: This is an Aspirational Guide</a></li>
</ul></li>
<li><a href="#2-repository-layout">2. Repository Layout</a>
<ul>
<li><a href="#example-layout">Example Layout</a></li>
<li><a href="#adapting-the-layout">Adapting the Layout</a></li>
</ul></li>
<li><a href="#3-the-specification-layer">3. The Specification Layer</a>
<ul>
<li><a href="#intent-specifications-nlspec">Intent Specifications
(NLSpec)</a></li>
<li><a href="#contracts">Contracts</a></li>
<li><a href="#constraints">Constraints</a></li>
</ul></li>
<li><a href="#4-scenarios-as-holdout-validation">4. Scenarios as Holdout
Validation</a>
<ul>
<li><a href="#the-holdout-set-analogy">The Holdout Set Analogy</a></li>
<li><a href="#what-makes-a-good-scenario">What Makes a Good
Scenario</a></li>
<li><a href="#storing-and-managing-holdout-scenarios">Storing and
Managing Holdout Scenarios</a></li>
<li><a href="#the-scenario-authoring-problem">The Scenario Authoring
Problem</a></li>
</ul></li>
<li><a href="#5-the-factory-development-loop">5. The Factory Development
Loop</a>
<ul>
<li><a href="#the-attractor-pattern">The Attractor Pattern</a></li>
<li><a href="#provider-aligned-agent-execution">Provider-Aligned Agent
Execution</a></li>
<li><a href="#convergence-and-the-digital-twin-universe">Convergence and
the Digital Twin Universe</a></li>
<li><a href="#the-satisfaction-metric">The Satisfaction Metric</a></li>
</ul></li>
<li><a href="#6-production-observability-and-specification-evolution">6.
Production Observability and Specification Evolution</a>
<ul>
<li><a href="#three-categories-of-production-signal">Three Categories of
Production Signal</a></li>
<li><a href="#the-current-state-of-production---specification-feedback">The
Current State of Production -&gt; Specification Feedback</a></li>
<li><a href="#what-a-production-feedback-loop-could-look-like">What a
Production Feedback Loop Could Look Like</a></li>
</ul></li>
<li><a href="#7-the-interactivenon-interactive-boundary">7. The
Interactive/Non-Interactive Boundary</a>
<ul>
<li><a href="#strongdms-shift-work-pattern">StrongDM&#39;s Shift Work
Pattern</a></li>
<li><a href="#when-is-the-specification-complete">When Is the
Specification Complete?</a></li>
<li><a href="#the-separate-agent-sets-architecture">The Separate Agent
Sets Architecture</a></li>
<li><a href="#industry-landscape">Industry Landscape</a></li>
</ul></li>
<li><a href="#8-evolving-specifications-over-time">8. Evolving
Specifications Over Time</a>
<ul>
<li><a href="#specification-versioning">Specification
Versioning</a></li>
<li><a href="#patterns-from-existing-systems">Patterns from Existing
Systems</a></li>
<li><a href="#the-spec-amendment-workflow">The Spec Amendment
Workflow</a></li>
</ul></li>
<li><a href="#9-soa-boundary-coordination">9. SOA Boundary
Coordination</a>
<ul>
<li><a href="#the-coordination-problem">The Coordination
Problem</a></li>
<li><a href="#out-of-scope">Out of Scope</a></li>
</ul></li>
<li><a href="#10-applicability-beyond-soa">10. Applicability Beyond
SOA</a></li>
<li><a href="#11-toolchain-summary">11. Toolchain Summary</a>
<ul>
<li><a href="#specification-authoring">Specification Authoring</a></li>
<li><a href="#factory-orchestration">Factory Orchestration</a></li>
<li><a href="#context-and-state">Context and State</a></li>
<li><a href="#agent-identity">Agent Identity</a></li>
<li><a href="#skills-and-capabilities">Skills and Capabilities</a></li>
<li><a href="#agentic-tui">Agentic TUI</a></li>
</ul></li>
<li><a href="#12-getting-started">12. Getting Started</a>
<ul>
<li><a href="#start-experimenting">Start experimenting</a></li>
<li><a href="#let-your-agents-read-this-guide">Let your agents read this
guide</a></li>
<li><a href="#notes-on-usage-of-kilroy">Notes on usage of
Kilroy</a></li>
</ul></li>
<li><a href="#13-out-of-scope">13. Out of Scope</a></li>
<li><a href="#14-open-questions">14. Open Questions</a>
<ul>
<li><a href="#specification-completeness-and-validation">Specification
Completeness and Validation</a></li>
<li><a href="#production-signals-and-specification-evolution">Production
Signals and Specification Evolution</a></li>
<li><a href="#scaling-and-complexity">Scaling and Complexity</a></li>
<li><a href="#enterprise-integration-and-auditability">Enterprise
Integration and Auditability</a></li>
<li><a href="#generated-code-as-artifact">Generated Code as
Artifact</a></li>
</ul></li>
<li><a href="#15-closing-perspective">15. Closing Perspective</a>
<ul>
<li><a href="#whats-next">What&#39;s next?</a></li>
</ul></li>
<li><a href="#references-and-further-reading">References and Further
Reading</a></li>
<li><a href="#appendix-a-personal-note">Appendix: A Personal
Note</a></li>
</ul>
<h2 id="executive-summary">Executive Summary</h2>
<p>The software factory pattern represents a fundamental shift in how
software is built: humans write specifications, coding agents produce
the implementation, and a separate validation process verifies
correctness — without human code review. In Dan Shapiro&#39;s five-level
taxonomy [<a href="#ref-1">1</a>], this is the Level 4-to-5 transition —
from human-as-engineering-manager to &quot;dark factory.&quot; This guide
describes the emerging mechanics for implementing that pattern within an
enterprise service-oriented architecture — what is known, what is
working, and what remains unsolved.</p>
<p><strong>The core idea.</strong> The repository encodes a fundamental
divide between what humans manage and what machines produce. A
&quot;context/intent engineer&quot; manages the human side: specifications (intent
describing what the software should do and why, contracts defining exact
API boundaries with neighboring services, constraints establishing
non-negotiable invariants), holdout scenarios for validation, factory
orchestration configuration, and documentation. The machine side is code
generated by agents — checked into version control for auditability, but
treated as opaque output whose correctness is verified exclusively
through externally observable behavior. The human&#39;s intellectual
contribution is entirely upstream of implementation.</p>
<p><strong>The key innovation: holdout scenario validation.</strong>
Borrowed from machine learning practice, scenarios used to evaluate the
software are kept hidden from the agents that build it. This prevents
reward-hacking — agents optimizing for tests rather than genuine
correctness. Separate agent sets enforce this: spec-refinement agents
help humans write specifications and scenarios but never see the code;
factory agents implement the code but never see the scenarios;
validation agents evaluate the built software but see neither source
code nor specifications.</p>
<p><strong>The factory loop.</strong> The Attractor pattern [<a href="#ref-2">2</a>], pioneered by StrongDM [<a href="#ref-3">3</a>],
orchestrates coding agents through a graph-structured pipeline of
implementation, testing, and refinement phases until the software
converges on a validated state. The graph is expressed in DOT format,
with each phase governed by targeted prompts and each transition
evaluated by the LLM. Different models can be used for different phases
— reasoning-heavy models for architecture, fast models for
boilerplate.</p>
<p><strong>Two shifts of work.</strong> The interactive shift is where
humans and agents collaborate on specifications, identify gaps, and
author scenarios. The non-interactive shift is where the factory runs
autonomously — potentially for hours — producing software without human
involvement. StrongDM calls this &quot;shift work&quot; [<a href="#ref-3">3</a>].
The boundary between these shifts is the central design decision:
determining when a specification is complete enough for autonomous
execution.</p>
<p><strong>What remains unsolved.</strong> Production observability
feeding back into specification evolution is the largest open problem —
no existing system fully closes this loop. Enterprise governance [<a href="#ref-4">4</a>], code provenance, cross-service contract
propagation, organizational transformation, and cost management are all
critical concerns acknowledged but intentionally scoped out.</p>
<p><strong>This is an aspirational guide.</strong> StrongDM [<a href="#ref-3">3</a>] has demonstrated the pattern at a startup with a
greenfield codebase. No large enterprise has publicly implemented it at
scale. <strong><em>I have not yet produced usable software with this
approach — early experiments have mainly revealed how hard it is to
define specifications with machine-executable rigor</em></strong>. The
guide describes what is becoming possible, grounded in published
experience from StrongDM, 8090 [<a href="#ref-5">5</a>], Superpowers [<a href="#ref-6">6</a>], GitHub&#39;s Spec Kit [<a href="#ref-7">7</a>], and my
own factory experiments using a forked implementation of Kilroy [<a href="#ref-8">8</a>] (a Go-based Attractor implementation). It is
versioned because we are all still learning, and there is more to
come.</p>
<hr />
<h2 id="audience-and-content">Audience and Content</h2>
<p>This paper is for people who want to learn about, create, and run
Software Factories. It&#39;s primarily aimed at more complex enterprise
environments, but any project of any size can use the information and
techniques presented.</p>
<p>It was researched, drafted, and edited with the help of AI — em
dashes and all — with significant manual curation, content additions,
and editing.</p>
<p>The personal note at the end, however, was entirely conceived and
written by a human.</p>
<p>It&#39;s long, but that is intentional.</p>
<p>The goal is to be a combination of &quot;here&#39;s some concrete details on
how you can actually start doing this&quot; and &quot;here&#39;s an AI-assisted brain
dump of everything I know about this so far with lots of references for
you to learn more&quot;.</p>
<p>Hopefully it is helpful to you :)</p>
<hr />
<h2 id="1-overview-and-scope">1. Overview and Scope</h2>
<p>You have a greenfield service to build within an existing enterprise
service-oriented architecture, and you want to build it using the
software factory pattern — where human-authored specifications drive
coding agents that produce, test, and converge working software without
human code review.</p>
<p>The factory pattern has moved from theoretical to operational.
StrongDM&#39;s AI team [<a href="#ref-3">3</a>] published the first detailed
public account of a production software factory in February 2026. A
three-person team produced security infrastructure — access management
software controlling permissions across Okta, Jira, Slack, and Google
Workspace — under two foundational rules: no human writes code, and no
human reviews code. The humans design specifications, curate validation
scenarios, and monitor satisfaction scores. The agents do everything
else.</p>
<p>Dan Shapiro&#39;s five-level taxonomy of AI-assisted programming [<a href="#ref-1">1</a>], published in January 2026, provides useful
positioning. Most development teams today operate at Level 2 (pair
programming with AI) or Level 3 (human as full-time code reviewer).
Level 4 shifts the human role to engineering management — writing specs,
crafting agent skills, reviewing plans, then stepping away while agents
execute. Level 5, the &quot;dark factory,&quot; removes human review entirely.
Industry data on agentic coding adoption [<a href="#ref-9">9</a>]
confirms this progression — most teams remain at Levels 2-3, with
multi-agent coordination patterns accelerating the transition toward
higher levels.</p>
<p>This guide targets the Level 4-to-5 transition for a single service.
It does not describe how an entire organization should restructure, nor
does it prescribe governance, compliance, or audit frameworks. Those
concerns matter enormously for enterprise adoption, but they are
separable from the core mechanics of turning human intent into working,
deployed software.</p>
<p><strong>Prerequisites assumed by this guide:</strong></p>
<p>Your organization already operates an SOA with defined service
boundaries, API contracts, and deployment infrastructure. The new
service has a clear domain bounded by contracts with neighboring
services. The team has access to frontier-class language models (Claude
Opus/Sonnet, GPT-5.x, or equivalent) and is willing to invest
meaningfully in token spend — StrongDM&#39;s benchmark of $1,000/day per
human engineer is aggressive, but directionally correct for Level 5
operation. At least one engineer with deep domain expertise will serve
as the specification author and factory operator.</p>
<p><strong>What this guide covers:</strong></p>
<p>The repository structure that separates human-authored specification
from machine-generated implementation. The specification layer — how to
express intent, contracts, and constraints in a form agents can execute
against. Scenarios as holdout validation — the mechanism that prevents
agents from reward-hacking their own tests. The factory development loop
— how the Attractor pattern uses DOT-based phase graphs to orchestrate
convergence. Production observability and its largely unsolved
relationship to specification evolution. The interactive/non-interactive
boundary — when human collaboration gives way to autonomous execution.
Patterns for evolving specifications over time, drawn from StrongDM,
8090 [<a href="#ref-5">5</a>], Superpowers [<a href="#ref-6">6</a>], and
GitHub&#39;s Spec Kit [<a href="#ref-7">7</a>].</p>
<h3 id="reality-check-this-is-an-aspirational-guide">Reality Check: This
is an Aspirational Guide</h3>
<p>This guide describes what is becoming possible, not what most people
are doing right now. StrongDM has demonstrated the approach at a startup
with a greenfield codebase and no customers yet. Other small and
mid-size companies are beginning to explore similar patterns. But as of
this writing, to my knowledge, no large enterprise has publicly shared
an implementation of factory-pattern development at scale. Sections <a href="#13-out-of-scope">13</a> and <a href="#14-open-questions">14</a>
list many large, critical barriers — established SDLC processes,
customers with SLAs, and significant legal and contractual commitments
among them.</p>
<p>My own early factory experiments have confirmed that this is
genuinely <em>HARD</em>. To break the fourth wall and be fully
transparent: <strong><em>I have not yet produced usable software with
the Software Factory approach, even of alpha quality.</em></strong>
After a week or two of hands-on experimentation, the main lessons have
been about what <em>doesn&#39;t</em> work — specifically, that you cannot
take shortcuts by relying too heavily on AI to generate the
specification. The difficulties are real:</p>
<ul>
<li>Defining specifications with machine-executable rigor, when you
can&#39;t rely on an experienced domain-aware human to &quot;know what you really
mean.&quot;</li>
<li>Creating the pipeline itself, with all of the quality gates you
would expect for a development and CI environment, but turned up to 11
and able to run in an automated feedback loop.</li>
<li>Defining validation harnesses and holdout scenarios with the rigor
to enforce an acceptable level of quality.</li>
</ul>
<p>You have to be deeply involved in creating the specification,
understand it thoroughly, and fully grasp how it drives factory
operation. This takes significant time and experience.</p>
<p>This guide is a starting milepost — one practitioner&#39;s attempt to
share what he has learned early in the journey, for the benefit of
others. It carries a version designation because it will evolve as the
community&#39;s collective understanding deepens. We are all learning
together how these ideas can and should work in practice.</p>
<hr />
<h2 id="2-repository-layout">2. Repository Layout</h2>
<h3 id="example-layout">Example Layout</h3>
<p>The repository structure encodes a fundamental distinction: what
humans manage (often with AI assistance) versus what machines produce
autonomously. Everything under <code>spec/</code>,
<code>holdout-scenarios/</code>, <code>factory/</code>, and
<code>docs/</code> is human-managed — authored or curated by the
context/intent engineer, possibly with agent assistance during
interactive sessions. Everything under <code>src/</code> is
machine-generated, version-controlled, but treated as opaque output
whose correctness is verified exclusively through externally observable
behavior.</p>
<pre><code>example-service/
├── spec/
│   ├── intent/
│   ├── contracts/
│   └── constraints/
├── holdout-scenarios/
├── factory/
│   ├── example-service.dot
│   └── attractor-config/
├── src/
└── docs/</code></pre>
<p><strong><code>spec/intent/</code></strong> — <strong>Intent.</strong>
The heart of the factory&#39;s input. The name &quot;intent&quot; was chosen
deliberately over &quot;requirements&quot; or &quot;features&quot; because it aligns with
the factory philosophy: humans express what the software should
accomplish and why, not how. The intent directory contains
natural-language specifications (NLSpec, borrowing StrongDM&#39;s
terminology) that describe the service&#39;s purpose, domain model,
behavioral narratives, and key architectural decisions. These are the
&quot;seed&quot; in StrongDM&#39;s Seed -&gt; Validation -&gt; Feedback loop.</p>
<p><strong><code>spec/contracts/</code></strong> —
<strong>Contracts.</strong> Defines the service&#39;s boundaries within the
SOA. For a greenfield service, these contracts are the most constrained
part of the specification — they must align precisely with the APIs of
neighboring services. Upstream contracts describe what this service can
expect from services it consumes. Downstream contracts define the APIs
this service exposes. These are not aspirational; they are hard
interfaces that the factory must implement exactly.</p>
<p><strong><code>spec/constraints/</code></strong> —
<strong>Constraints.</strong> Holds what GitHub&#39;s Spec Kit [<a href="#ref-7">7</a>] calls the &quot;constitution&quot; — immutable principles
that apply to every change. SLO targets, security requirements, and
operational constraints live here. Unlike intent, which evolves as
understanding deepens, constraints tend to be stable and non-negotiable.
An SLO of p99 latency under 200ms is not a suggestion the agent can
reinterpret; it is a hard boundary.</p>
<p><strong><code>holdout-scenarios/</code></strong> —
<strong>Validation.</strong> Stored outside the main codebase accessible
to factory agents. The separation mechanism can be sparse checkout
rules, a separate repository with restricted access, or an external
system. The critical property: factory agents during implementation
never see these scenarios. They function as a holdout set in the machine
learning sense, preventing agents from overfitting their implementation
to the test data. More on this in <a href="#4-scenarios-as-holdout-validation">Section 4</a>.</p>
<p><strong><code>factory/</code></strong> —
<strong>Orchestration.</strong> Contains the Attractor phase graph (a
DOT file defining the generative SDLC) and its configuration. The phase
graph is the factory&#39;s &quot;program&quot; — it defines what phases the coding
agents traverse, what prompts govern each phase, and what convergence
criteria determine when the software is ready. Human-authored,
version-controlled.</p>
<p><strong><code>src/</code></strong> — <strong>Output.</strong>
Contains machine-generated code. <code>src</code> is a placeholder name
— it represents any artifacts generated by the factory, including tests,
configuration, and infrastructure-as-code. It is checked into version
control. Sandgarden [<a href="#ref-10">10</a>] takes the opposite
approach, treating source code as generated output that is never checked
in. This guide treats Sandgarden&#39;s no-check-in model as a non-goal: for
enterprise systems, checked-in code provides auditability, rollback
capability, and integration with existing CI/CD pipelines. The code is
what StrongDM calls &quot;opaque weights whose correctness is inferred
exclusively from externally observable behavior.&quot; You version it for
operational reasons, not because humans will read it for
comprehension.</p>
<p><strong><code>docs/</code></strong> — <strong>Documentation.</strong>
Represents human-facing content — developer documentation on the
codebase or factory operation. There may be other human-facing content
in the repo, depending on the project. Some will be human-authored, some
agent-generated, some mixed.</p>
<h3 id="adapting-the-layout">Adapting the Layout</h3>
<p>The example layout above is illustrative, not prescriptive. The
actual structure of your specification, holdout scenarios, and
supporting context will vary across projects, teams, and planning
processes. A team building a data pipeline may organize intent around
transformation stages rather than user journeys. A team with an existing
test harness may store holdout scenarios in a separate repository rather
than a sibling directory.</p>
<p>What the layout does express is the fundamental divide at the heart
of the factory pattern: work that humans manage (often with AI
assistance) versus work that machines perform autonomously. StrongDM
calls this &quot;shift work&quot; (explored in detail in <a href="#7-the-interactivenon-interactive-boundary">Section 7</a>) — the
interactive shift where humans and agents collaborate on specification
and scenarios, and the non-interactive shift where the factory generates
code without human involvement. The directories above fall on one side
or the other. <code>spec/</code>, <code>holdout-scenarios/</code>,
<code>factory/</code>, and <code>docs/</code> are all human-managed
outputs of the interactive shift. <code>src/</code> is the output of the
non-interactive shift. <code>factory/</code> bridges the two —
human-authored, but it defines how the non-interactive shift runs. The
example layout was chosen to make this divide concrete so that the rest
of this guide can reference it clearly.</p>
<p><strong>A note on the single human role.</strong> In this model,
there is one human function: the context/intent engineer. This person
(or small team) authors specifications, writes scenarios, configures the
factory, and monitors production signals. They do not write code. They
do not review code. Their intellectual contribution is upstream of
implementation — defining what should exist and how to verify it exists
correctly.</p>
<hr />
<h2 id="3-the-specification-layer">3. The Specification Layer</h2>
<p>The specification layer is the factory&#39;s primary input and the
human&#39;s primary artifact. Getting specifications right is where the hard
intellectual work lives. As GitHub&#39;s Spec Kit documentation puts it:
&quot;The lingua franca of development moves to a higher level, and code is
the last-mile approach.&quot;</p>
<p>Böckeler&#39;s analysis of spec-driven development tooling [<a href="#ref-11">11</a>] identifies three levels of increasing commitment
to specification as the central artifact. <strong>Spec-first</strong>: a
specification is written before implementation, then discarded once the
task is complete. <strong>Spec-anchored</strong>: the specification
persists after implementation, serving as a living reference for
evolution and maintenance. <strong>Spec-as-source</strong>: the
specification becomes the primary source file — humans edit only the
spec, never the code. The factory pattern described in this guide is an
implementation of spec-as-source: the <code>spec/</code> directory is
the human&#39;s artifact, <code>src/</code> is generated output, and the
factory mediates the transformation between them.</p>
<h3 id="intent-specifications-nlspec">Intent Specifications
(NLSpec)</h3>
<p>StrongDM coined the term NLSpec — natural-language specification — to
describe the markdown documents that seed their factory. An NLSpec is
not a traditional requirements document, a user story, or a PRD, though
it shares DNA with all three. It is precise enough that a coding agent
can implement against it, yet written in natural language that a domain
expert can author and review without programming knowledge.</p>
<p>An effective intent specification answers three questions for every
significant behavior: what should happen, under what conditions, and why
this matters to the user. The &quot;why&quot; is not decoration — it gives the
agent context for making reasonable decisions when the specification is
silent on edge cases. Compare: &quot;users must authenticate before accessing
resources&quot; versus &quot;users must authenticate before accessing resources
because this service manages sensitive permission data, and
unauthenticated access would violate our security model&#39;s zero-trust
boundary.&quot; The second version helps an agent infer that, when
encountering an ambiguous case, it should fail closed rather than
open.</p>
<p>The intent directory is organized around behavioral narratives rather
than technical components. User journeys describe end-to-end flows from
the perspective of the service&#39;s consumers (which, in an SOA, may be
other services rather than humans). Decision records capture significant
architectural choices and their rationale, giving agents context for why
the system is shaped a particular way.</p>
<h3 id="contracts">Contracts</h3>
<p>For a service within an SOA, contracts are the hardest constraint.
They are the exact interfaces that neighboring services already
implement or expect. A contract specification includes the API schema
(OpenAPI, protobuf, GraphQL, or equivalent), expected request/response
patterns, error handling conventions, authentication mechanisms, and
rate limiting behavior.</p>
<p>Contracts are bidirectional. Upstream contracts describe what this
service can rely on from services it consumes — their APIs, their SLAs,
their failure modes. Downstream contracts define what this service
promises to its consumers. The factory must produce an implementation
that satisfies all downstream contracts while correctly consuming all
upstream contracts.</p>
<p>In an enterprise SOA, contract specifications often already exist —
API documentation, interface definition files, or integration test
suites. The specification engineer&#39;s job is to capture these in
<code>spec/contracts/</code> in a form the factory agents can consume.
This may involve translating existing OpenAPI specs into annotated
NLSpec that adds behavioral context beyond the schema definition.</p>
<h3 id="constraints">Constraints</h3>
<p>Constraints are the non-negotiable invariants that the factory must
satisfy regardless of implementation approach. They include
service-level objectives (latency percentiles, error budgets, throughput
targets), security requirements (authentication mechanisms, data
encryption standards, authorization models), and operational constraints
(memory limits, CPU budgets, deployment topology requirements).</p>
<p>Constraints function as the &quot;constitution&quot; — immutable, applied to
every change. The factory&#39;s validation harness checks generated code
against constraints continuously, not just at the end of a development
cycle.</p>
<p>A well-written constraint is measurable. &quot;The service should be fast&quot;
is not a constraint. &quot;P99 response latency must remain below 200ms for
the /permissions endpoint under sustained load of 1,000 requests per
second&quot; is.</p>
<hr />
<h2 id="4-scenarios-as-holdout-validation">4. Scenarios as Holdout
Validation</h2>
<p>The scenario holdout mechanism is the most important innovation in
the factory pattern, and it emerged from a painful discovery. When
StrongDM&#39;s team began building software with coding agents, they hit the
classic reward-hacking problem: agents tasked with making tests pass
would write <code>return true</code>. Perfectly rational strategy for
passing a narrowly written test. Produces useless software.</p>
<p>Traditional testing approaches — unit tests, integration tests,
end-to-end tests — all share a vulnerability when agents write both the
implementation and the tests: the agent can optimize for the test rather
than for the intended behavior. This is not hypothetical. It happened
repeatedly in StrongDM&#39;s early experiments. Tests that lived inside the
codebase, visible to the coding agents, became targets for optimization
rather than genuine quality signals.</p>
<h3 id="the-holdout-set-analogy">The Holdout Set Analogy</h3>
<p>The solution borrows directly from machine learning practice. In
model training, you partition your data into training and evaluation
sets. The model never sees the evaluation data during training,
preventing it from memorizing answers rather than learning generalizable
patterns. StrongDM applied the same principle to software validation:
scenarios used to evaluate the software are stored where the coding
agents cannot access them during implementation.</p>
<p>Simon Willison [<a href="#ref-12">12</a>], visiting the StrongDM team
in October 2025, described this as imitating &quot;aggressive testing by an
external QA team — an expensive but highly effective way of ensuring
quality in traditional software.&quot; The analogy is apt. In traditional
development, the most rigorous quality assurance comes from testers
deliberately isolated from the development process — they test against
requirements, not implementation details. The holdout scenario set
creates this separation structurally.</p>
<h3 id="what-makes-a-good-scenario">What Makes a Good Scenario</h3>
<p>StrongDM repurposed the term &quot;scenario&quot; (inspired by Cem Kaner&#39;s
scenario testing, 2003 [<a href="#ref-13">13</a>]) to mean an end-to-end
user story that can be &quot;intuitively understood and flexibly validated by
an LLM.&quot; A scenario is not a unit test with a precise assertion. It is a
narrative description of what a user (or consuming service) does, what
should happen, and what constitutes a satisfactory outcome.</p>
<p>This narrative quality is essential because the validation is
probabilistic, not boolean. StrongDM uses &quot;satisfaction&quot; to quantify
scenario validation: across all observed trajectories through all
scenarios, what fraction likely satisfies the user? This is a paradigm
shift from traditional testing&#39;s green/red binary. A scenario might be
satisfied 95% of the time across multiple runs — and for agentic
software with inherent non-determinism, that may be perfectly
acceptable.</p>
<p>Good scenarios span three categories: happy-path scenarios describing
the normal expected flow, edge-case scenarios probing boundary
conditions and unusual-but-valid usage patterns, and failure-mode
scenarios verifying graceful degradation when dependencies fail, inputs
are malformed, or resources are exhausted.</p>
<h3 id="storing-and-managing-holdout-scenarios">Storing and Managing
Holdout Scenarios</h3>
<p>The separation between scenarios and the implementation codebase can
be achieved through several mechanisms.</p>
<p><strong>Separate repository with restricted access</strong> is the
most straightforward. The holdout scenarios live in a different
repository that factory agents have no credentials to access. A separate
validation harness pulls scenarios from this repository and runs them
against the built software.</p>
<p><strong>Sparse checkout or access controls within the same
repository</strong> can work if the VCS supports fine-grained access.
The factory agents&#39; credentials grant access only to <code>spec/</code>
and <code>src/</code>, not to <code>holdout-scenarios/</code>. Simpler
operationally, but requires confident access control.</p>
<p><strong>External system</strong> — a scenario management service or
database that the validation harness queries at evaluation time. More
infrastructure complexity, but the strongest separation guarantee and
independent scenario versioning.</p>
<p>What matters is the invariant: during implementation, factory agents
must not have access to holdout scenarios. During validation, a separate
process evaluates the implementation against those scenarios.</p>
<h3 id="the-scenario-authoring-problem">The Scenario Authoring
Problem</h3>
<p>How do you write good scenarios efficiently? If the human
specification engineer must author every scenario by hand, the process
bottlenecks on human throughput. If agents help write scenarios, you
risk circularity — agents might write scenarios that are easy to satisfy
rather than scenarios that genuinely test correctness.</p>
<p>The emerging answer involves separate agent sets. Spec-refinement
agents (operating interactively with the human, with access to the
intent specification) can brainstorm and draft scenarios. But these
agents must be architecturally separate from the factory execution
agents that implement the software. The spec-refinement agents never see
the implementation; the factory agents never see the scenarios. This
separation prevents either agent set from gaming the process.</p>
<p>The concrete mechanics of this separation are still being worked out.
StrongDM has demonstrated it works in practice with their team of three.
How it scales to larger organizations with more complex services remains
an active area of exploration.</p>
<h3 id="the-scenario-execution-problem">The Scenario Execution
Problem</h3>
<p>AI-assisted scenario validation (e.g., driving a Playwright MCP
server against a running app) requires strict isolation. Validation
agents must interact only with the built artifact as a running service —
never with implementation source code. Without this boundary, agents
with code access will attempt to fix failures rather than report them,
defeating the purpose of holdout validation.</p>
<p>Similarly, automating the feedback loop from validation failures back
into the specification requires care. Unchecked automation here is a
path for AI slop to contaminate your carefully curated
specifications.</p>
<p>The <a href="#the-separate-agent-sets-architecture">Separate Agent
Sets Architecture</a> defines the access boundaries that enforce this
isolation.</p>
<hr />
<h2 id="5-the-factory-development-loop">5. The Factory Development
Loop</h2>
<p>The factory development loop is the core engine that transforms
specifications into working software. At its heart is the Attractor
pattern — a graph-structured pipeline that orchestrates coding agents
through phases of implementation, testing, and refinement until the
software converges on a state that satisfies all specifications and
passes holdout validation.</p>
<h3 id="the-attractor-pattern">The Attractor Pattern</h3>
<p>StrongDM&#39;s Attractor [<a href="#ref-2">2</a>] is a non-interactive
coding agent structured as a directed graph of phases. Each node
corresponds to a development phase — &quot;implement the functionality,&quot;
&quot;write integration tests,&quot; &quot;identify and fix the performance
bottleneck,&quot; &quot;refactor for readability&quot; — and is governed by a core
prompt defining the phase&#39;s objective. Edges between nodes are expressed
in natural language and evaluated by the LLM: the agent examines the
current codebase state, the phase&#39;s completion criteria, and decides
which edge to follow next.</p>
<p>The graph is expressed in DOT format (Graphviz&#39;s graph description
language). Jesse Vincent&#39;s experiments with Superpowers [<a href="#ref-6">6</a>] confirmed that frontier LLMs can interpret DOT
graphs as workflow instructions effectively, and StrongDM built their
entire factory orchestration around this capability. A simplified
Attractor graph for a single service (you can feed this definition into
an LLM to have it render the graph):</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode dot"><code class="sourceCode dot"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="kw">digraph</span> <span class="va">factory</span> <span class="ot">{</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="co">    </span><span class="at">rankdir</span><span class="ot">=</span><span class="va">TB</span><span class="ot">;</span></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a><span class="co">    </span><span class="va">seed</span><span class="co"> </span><span class="ot">[</span><span class="at">label</span><span class="ot">=</span><span class="st">&quot;Load Specification\n(intent + contracts + constraints)&quot;</span><span class="ot">];</span></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a><span class="co">    </span><span class="va">scaffold</span><span class="co"> </span><span class="ot">[</span><span class="at">label</span><span class="ot">=</span><span class="st">&quot;Generate Project Scaffold\n(build, dirs, CI)&quot;</span><span class="ot">];</span></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a><span class="co">    </span><span class="va">implement</span><span class="co"> </span><span class="ot">[</span><span class="at">label</span><span class="ot">=</span><span class="st">&quot;Implement Core Functionality\n(from intent spec)&quot;</span><span class="ot">];</span></span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a><span class="co">    </span><span class="va">contracts</span><span class="co"> </span><span class="ot">[</span><span class="at">label</span><span class="ot">=</span><span class="st">&quot;Implement Contract Interfaces\n(consumers, downstream APIs)&quot;</span><span class="ot">];</span></span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a><span class="co">    </span><span class="va">unit_test</span><span class="co"> </span><span class="ot">[</span><span class="at">label</span><span class="ot">=</span><span class="st">&quot;Generate Unit Tests\n(from spec, not implementation)&quot;</span><span class="ot">];</span></span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a><span class="co">    </span><span class="va">integration</span><span class="co"> </span><span class="ot">[</span><span class="at">label</span><span class="ot">=</span><span class="st">&quot;Integration Testing\n(against DTU or stubs)&quot;</span><span class="ot">];</span></span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a><span class="co">    </span><span class="va">constraints_check</span><span class="co"> </span><span class="ot">[</span><span class="at">label</span><span class="ot">=</span><span class="st">&quot;Validate Constraints\n(SLOs, security, ops limits)&quot;</span><span class="ot">];</span></span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a><span class="co">    </span><span class="va">refine</span><span class="co"> </span><span class="ot">[</span><span class="at">label</span><span class="ot">=</span><span class="st">&quot;Refine Implementation\n(fix failures, optimize)&quot;</span><span class="ot">];</span></span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a><span class="co">    </span><span class="va">converged</span><span class="co"> </span><span class="ot">[</span><span class="at">label</span><span class="ot">=</span><span class="st">&quot;Convergence Check\n(all green, threshold met)&quot;</span><span class="ot">];</span></span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a><span class="co">    </span><span class="va">done</span><span class="co"> </span><span class="ot">[</span><span class="at">label</span><span class="ot">=</span><span class="st">&quot;Output: Validated Build Artifact&quot;</span><span class="ot">];</span></span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a><span class="co">    </span><span class="va">seed</span><span class="co"> </span><span class="ot">-&gt;</span><span class="co"> </span><span class="va">scaffold</span><span class="ot">;</span></span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a><span class="co">    </span><span class="va">scaffold</span><span class="co"> </span><span class="ot">-&gt;</span><span class="co"> </span><span class="va">implement</span><span class="ot">;</span></span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a><span class="co">    </span><span class="va">implement</span><span class="co"> </span><span class="ot">-&gt;</span><span class="co"> </span><span class="va">contracts</span><span class="ot">;</span></span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a><span class="co">    </span><span class="va">contracts</span><span class="co"> </span><span class="ot">-&gt;</span><span class="co"> </span><span class="va">unit_test</span><span class="ot">;</span></span>
<span id="cb2-19"><a href="#cb2-19" aria-hidden="true" tabindex="-1"></a><span class="co">    </span><span class="va">unit_test</span><span class="co"> </span><span class="ot">-&gt;</span><span class="co"> </span><span class="va">integration</span><span class="ot">;</span></span>
<span id="cb2-20"><a href="#cb2-20" aria-hidden="true" tabindex="-1"></a><span class="co">    </span><span class="va">integration</span><span class="co"> </span><span class="ot">-&gt;</span><span class="co"> </span><span class="va">constraints_check</span><span class="ot">;</span></span>
<span id="cb2-21"><a href="#cb2-21" aria-hidden="true" tabindex="-1"></a><span class="co">    </span><span class="va">constraints_check</span><span class="co"> </span><span class="ot">-&gt;</span><span class="co"> </span><span class="va">converged</span><span class="ot">;</span></span>
<span id="cb2-22"><a href="#cb2-22" aria-hidden="true" tabindex="-1"></a><span class="co">    </span><span class="va">converged</span><span class="co"> </span><span class="ot">-&gt;</span><span class="co"> </span><span class="va">done</span><span class="co"> </span><span class="ot">[</span><span class="at">label</span><span class="ot">=</span><span class="st">&quot;converged&quot;</span><span class="ot">];</span></span>
<span id="cb2-23"><a href="#cb2-23" aria-hidden="true" tabindex="-1"></a><span class="co">    </span><span class="va">converged</span><span class="co"> </span><span class="ot">-&gt;</span><span class="co"> </span><span class="va">refine</span><span class="co"> </span><span class="ot">[</span><span class="at">label</span><span class="ot">=</span><span class="st">&quot;not converged&quot;</span><span class="ot">];</span></span>
<span id="cb2-24"><a href="#cb2-24" aria-hidden="true" tabindex="-1"></a><span class="co">    </span><span class="va">refine</span><span class="co"> </span><span class="ot">-&gt;</span><span class="co"> </span><span class="va">implement</span><span class="co"> </span><span class="ot">[</span><span class="at">label</span><span class="ot">=</span><span class="st">&quot;functional issues&quot;</span><span class="ot">];</span></span>
<span id="cb2-25"><a href="#cb2-25" aria-hidden="true" tabindex="-1"></a><span class="co">    </span><span class="va">refine</span><span class="co"> </span><span class="ot">-&gt;</span><span class="co"> </span><span class="va">contracts</span><span class="co"> </span><span class="ot">[</span><span class="at">label</span><span class="ot">=</span><span class="st">&quot;contract violations&quot;</span><span class="ot">];</span></span>
<span id="cb2-26"><a href="#cb2-26" aria-hidden="true" tabindex="-1"></a><span class="co">    </span><span class="va">refine</span><span class="co"> </span><span class="ot">-&gt;</span><span class="co"> </span><span class="va">unit_test</span><span class="co"> </span><span class="ot">[</span><span class="at">label</span><span class="ot">=</span><span class="st">&quot;test gaps&quot;</span><span class="ot">];</span></span>
<span id="cb2-27"><a href="#cb2-27" aria-hidden="true" tabindex="-1"></a><span class="ot">}</span></span></code></pre></div>
<p>This is deliberately simplified. A production Attractor graph has
more phases, more feedback edges, and phase-specific prompts that can
run for many iterations. The key properties: the graph is acyclic at the
macro level (progressing from specification to validated artifact) with
controlled cycles for refinement (convergence loops), and every edge
condition is evaluable by an LLM examining the current state.</p>
<h3 id="provider-aligned-agent-execution">Provider-Aligned Agent
Execution</h3>
<p>A critical implementation detail from StrongDM&#39;s Attractor
specification: different LLM providers perform best with their native
tool interfaces. Anthropic&#39;s models work best with Claude Code&#39;s editing
tools and system prompts. OpenAI&#39;s models work best with codex-rs&#39;s tool
schemas. Gemini models work best with gemini-cli&#39;s tool set. The
Attractor spec is explicit: &quot;The initial base for each provider should
be a 1:1 copy of the provider&#39;s reference agent — the exact same system
prompt, the exact same tool definitions, byte for byte. Not a similar
prompt. Not similar tools.&quot;</p>
<p>This is not aesthetic preference. Models are trained and optimized
against specific tool interfaces, and forcing a universal toolset
degrades performance.</p>
<p>For enterprise teams, this means the factory&#39;s execution layer should
abstract over provider-specific agents. Each phase in the graph can use
a different model — a reasoning-heavy model for architecture decisions,
a fast model for boilerplate generation, a specialized model for
security review. The Attractor&#39;s graph structure makes this composition
natural: each node specifies its model and tool configuration
independently.</p>
<h3 id="convergence-and-the-digital-twin-universe">Convergence and the
Digital Twin Universe</h3>
<p>Convergence means the factory has produced software that satisfies
all specifications and passes holdout validation at or above the
satisfaction threshold. Getting there requires a validation environment
that can exercise the software realistically and repeatedly.</p>
<p>StrongDM&#39;s Digital Twin Universe (DTU) is the most ambitious example
— behavioral clones of every third-party service the software depends
on. Full replicas of Okta&#39;s API, Jira&#39;s API, Slack&#39;s API, Google
Workspace&#39;s APIs, including edge cases and failure modes. The DTU
enables validation at volumes far exceeding production limits, with
deterministic and replayable conditions, without hitting rate limits or
accumulating API costs.</p>
<p>Building a DTU at this fidelity is a significant investment.
StrongDM&#39;s insight: the DTU itself can be built by coding agents. Dump a
service&#39;s public API documentation into an agent and have it build a
behavioral clone as a self-contained binary. For services within your
own SOA, you may already have contract test suites or API simulators
that serve as a foundation.</p>
<p>For teams starting smaller, the DTU concept scales down. At minimum,
you need contract stubs for upstream services (mocking their APIs
according to your <code>spec/contracts/</code> definitions) and a test
harness that exercises your service&#39;s downstream APIs. The holdout
scenarios run against this environment. As the factory matures, invest
in higher-fidelity twins.</p>
<h3 id="the-satisfaction-metric">The Satisfaction Metric</h3>
<p>Traditional software validation asks: &quot;Do all tests pass?&quot; The
factory pattern asks: &quot;Across all observed trajectories through all
scenarios, what fraction likely satisfies the user?&quot;</p>
<p>This shift from boolean to probabilistic validation is necessary
because the factory produces software with agentic components — code
that may interact with LLMs, handle non-deterministic inputs, or make
decisions under uncertainty. A scenario might be satisfied 97 times out
of 100 runs. Whether 97% is acceptable depends on the scenario&#39;s
criticality and the service&#39;s SLO requirements.</p>
<p>The satisfaction metric is computed by a separate validation agent
(or agent set) that has access to the holdout scenarios but not the
implementation source. This agent runs scenarios against the built
artifact, observes outcomes, and reports a satisfaction score. The
Attractor&#39;s convergence check compares this score against the threshold
defined in the specification&#39;s constraints.</p>
<hr />
<h2 id="6-production-observability-and-specification-evolution">6.
Production Observability and Specification Evolution</h2>
<p>What happens after the factory produces software and that software
runs in production? How do production signals flow back to influence
specifications? This is the least-solved problem in the factory pattern,
and we present it as frontier territory rather than settled
practice.</p>
<h3 id="three-categories-of-production-signal">Three Categories of
Production Signal</h3>
<p>Production signals that should influence specifications fall into
three categories, each requiring different detection and response.</p>
<p><strong>Signal Type A: Behavioral Drift.</strong> The service&#39;s
actual behavior diverges from the specification. The code was correct
when the factory produced it, but something changed — a dependency
updated its API, traffic patterns shifted, a configuration was modified.
Detection: contract testing against live upstream services, scenario
replay against production (using a subset of holdout scenarios safe for
production execution), anomaly detection on telemetry, SLO monitoring.
The response is usually mechanical: re-run the factory with the current
specification to produce an updated build. This is the easiest signal to
automate because the specification itself hasn&#39;t failed — the
implementation just needs to be re-converged.</p>
<p><strong>Signal Type B: Specification Gaps.</strong> The service
encounters situations the specification never contemplated. A new edge
case. A consuming service sending unexpected request patterns. An
unmodeled failure mode. Detection is harder — it surfaces as errors or
behaviors that don&#39;t map to any known scenario. Log analysis, trace
inspection, and anomaly detection can surface these signals, but
interpreting them requires judgment. The response requires human
involvement: the specification engineer examines the gap, decides
whether to amend intent, add scenarios, update constraints, or document
the case as out-of-scope. This cannot be fully automated because it
requires understanding whether the gap matters.</p>
<p><strong>Signal Type C: Satisfaction Regression.</strong> The service
meets its specification perfectly, but users (or consuming services) are
unhappy. The specification itself is wrong — it describes behavior that
doesn&#39;t serve the service&#39;s purpose. This is the hardest signal. It
surfaces through fuzzy channels: support tickets, user feedback, SLO
breaches that technically stay within budget but feel wrong, consumption
patterns suggesting workarounds. The response is creative: the
specification engineer must evolve the intent itself, which may cascade
through contracts, constraints, and scenarios. This is a product and
design problem, not an engineering problem, and no automation replaces
the human judgment required.</p>
<h3 id="the-current-state-of-production---specification-feedback">The
Current State of Production -&gt; Specification Feedback</h3>
<p>No existing system fully closes the loop from production
observability to specification evolution in an automated way. The
closest implementations are partial.</p>
<p><strong>8090&#39;s Validator module [<a href="#ref-5">5</a>]</strong>
converts user feedback into actionable development tasks, creating a
pipeline from usage back into the build process. But it focuses on
explicit user feedback, not production telemetry, and the &quot;actionable
tasks&quot; still require human interpretation before becoming specification
changes.</p>
<p><strong>Microsoft&#39;s AI-led SDLC prototype [<a href="#ref-14">14</a>]</strong> combines Spec Kit for spec-driven
development with an SRE agent that proactively monitors production and
opens issues automatically. Quick fixes (incorrect port configuration,
scaling adjustments) can be assigned directly to a coding agent. But the
SRE agent opens issues — it does not amend specifications. The human
still bridges that gap.</p>
<p><strong>Observability-Driven Development (ODD)</strong>, as
articulated by Charity Majors and the Honeycomb team, provides the
theoretical framework: weave observability into development so
production behavior continuously informs design decisions. The Stack
Overflow engineering blog&#39;s process capability model [<a href="#ref-15">15</a>] — defining upper and lower specification limits
and measuring production against them — maps directly to SLOs and error
budgets. But ODD was formulated for human developers, not for
factory-pattern systems where the &quot;developer&quot; is an agent.</p>
<h3 id="what-a-production-feedback-loop-could-look-like">What a
Production Feedback Loop Could Look Like</h3>
<p>We can sketch what an ideal production feedback loop would include,
even though no existing system implements it end-to-end.</p>
<p>Signal Type A (behavioral drift) could be detected automatically
through continuous contract testing and scenario replay. When drift is
detected, the factory re-runs to produce an updated build. This is
achievable today — it is essentially a CI/CD pipeline that triggers on
production anomalies rather than code changes.</p>
<p>Signal Type B (spec gaps) requires a triage layer. Production
anomalies that don&#39;t match known scenarios are surfaced to the
specification engineer with context: the triggering request, relevant
traces, the gap between observed and expected behavior. The engineer
decides whether to amend the spec. An agent could draft a proposed
amendment and new scenarios, but the human must approve.</p>
<p>Signal Type C (satisfaction regression) requires instrumentation
beyond traditional observability — measuring user satisfaction directly
through usage patterns, feedback mechanisms, and correlation with
business outcomes, then surfacing that data in a form that informs
specification evolution. This is product analytics territory, and
connecting it to the factory&#39;s specification layer is genuinely hard.
StrongDM&#39;s principle of &quot;Apply More Tokens&quot; — converting obstacles into
representations models can understand — applies here, but the concrete
mechanisms are still being invented.</p>
<p><strong>Important architectural note:</strong> CXDB (StrongDM&#39;s
context database) is the factory&#39;s internal context store — it records
agent conversations, tool outputs, and convergence history for the
factory&#39;s own use. It is not the bridge for production signals.
Production observability requires its own telemetry pipeline (metrics,
logs, traces via OpenTelemetry or equivalent) feeding into the triage
layer described above. Separate systems, different purposes.</p>
<hr />
<h2 id="7-the-interactivenon-interactive-boundary">7. The
Interactive/Non-Interactive Boundary</h2>
<p><a href="#2-repository-layout">Section 2</a> introduced the shift
work divide — the distinction between human-managed interactive work and
autonomous factory execution — as the organizing principle behind the
repository layout. This section examines that boundary in depth, because
where exactly interactive collaboration ends and autonomous execution
begins is the central unsolved problem in factory-pattern development.
We do not have a definitive answer. What we offer is a landscape of
approaches, a concrete framing from StrongDM, and honest identification
of the gaps.</p>
<h3 id="strongdms-shift-work-pattern">StrongDM&#39;s Shift Work Pattern</h3>
<p>StrongDM names this directly in their Techniques documentation (shift
work technique [<a href="#ref-3">3</a>]): &quot;Separate interactive work
from fully specified work. When intent is complete (specs, tests,
existing apps), an agent can run end-to-end without back-and-forth.&quot;</p>
<p>The term &quot;shift work&quot; evokes a factory floor where different shifts
perform different kinds of work.</p>
<p><strong>The interactive shift</strong> is where humans and agents
collaborate on specification. The human describes intent, the agent asks
clarifying questions, proposes structures, identifies gaps, drafts
specification language. This is Shapiro&#39;s Level 4 activity: &quot;You write a
spec. You argue with it about the spec.&quot; The output is a complete,
internally consistent specification (intent + contracts + constraints)
and a set of holdout scenarios. The interactive shift uses an agentic
TUI (terminal user interface) as the primary collaboration tool — BMAD&#39;s
planning agents [<a href="#ref-16">16</a>] (Analyst, PM, Architect),
Superpowers&#39; brainstorm -&gt; plan workflow, or any coding agent&#39;s
conversational mode.</p>
<p><strong>The non-interactive shift</strong> is where the Attractor
runs. The specification is complete. The scenarios are written. The
factory graph executes end-to-end — agents implement, test, refine, and
converge without human intervention. This shift may run for hours or
overnight. The human checks the satisfaction score when it finishes.</p>
<p>The boundary between these shifts is the critical question: when is a
specification &quot;complete enough&quot; for non-interactive execution?</p>
<h3 id="when-is-the-specification-complete">When Is the Specification
Complete?</h3>
<p>No formal criterion exists. Completeness is a judgment call informed
by experience. But several heuristics help.</p>
<p><strong>The new-hire test.</strong> GitHub&#39;s Spec Kit [<a href="#ref-7">7</a>] and Factory [<a href="#ref-17">17</a>] both
suggest: &quot;Would a capable new hire, given this specification and no
other context, implement correctly without interrupting you more than
once?&quot; If no, the specification has gaps that will cause the factory to
make wrong assumptions.</p>
<p><strong>Scenario coverage.</strong> If you can write holdout
scenarios for every behavioral narrative in the intent specification and
every edge case in the contracts, the specification is likely complete
enough. If you find yourself unable to write a scenario because you
don&#39;t know what the correct behavior should be — that&#39;s a specification
gap that needs interactive resolution.</p>
<p><strong>Contract precision.</strong> If the upstream and downstream
API specifications are precise enough that you could hand-write an
integration test against them, they&#39;re complete enough for the
factory.</p>
<p><strong>Constraint measurability.</strong> Every constraint should
have a corresponding measurement. If a constraint can&#39;t be measured by
the validation harness, it either needs to be made measurable or
acknowledged as aspirational guidance rather than a factory input.</p>
<h3 id="the-separate-agent-sets-architecture">The Separate Agent Sets
Architecture</h3>
<p>A key architectural insight prevents circularity: the agents that
help refine specifications during the interactive shift must be separate
from the agents that implement during the non-interactive shift.</p>
<p><strong>Spec-refinement agents</strong> operate interactively with
the human. They have access to intent, contracts, and constraints. They
can see and help author holdout scenarios. They assist with
brainstorming, gap analysis, and specification language. They never see
the implementation code.</p>
<p><strong>Factory execution agents</strong> (the Attractor) operate
non-interactively. They have access to intent, contracts, and
constraints. They produce implementation code and internal tests. They
never see the holdout scenarios.</p>
<p><strong>Validation agents</strong> operate during holdout evaluation.
They have access to holdout scenarios and the built artifact (as a
running service, not source code). They run scenarios, observe outcomes,
and compute satisfaction scores. They never see implementation source or
the specification.</p>
<p>This three-way separation ensures no agent set can game the process.
Spec-refinement agents can&#39;t write easy-to-satisfy scenarios because
they don&#39;t know how the code will be structured. Factory agents can&#39;t
write implementations that merely pass tests because they can&#39;t see the
scenarios. Validation agents can&#39;t inflate scores because they only
observe behavior, not code.</p>
<h3 id="industry-landscape">Industry Landscape</h3>
<p>The interactive/non-interactive boundary manifests differently across
existing tools.</p>
<p><strong>BMAD</strong> has the clearest phase structure: Analysis
-&gt; Planning -&gt; Solutioning -&gt; Implementation. The first three
phases are interactive (human + planning agents), Implementation is
increasingly autonomous. But BMAD doesn&#39;t formalize the handoff or
enforce separation between planning and implementation agent sets.</p>
<p><strong>Superpowers</strong> enforces a brainstorm -&gt; plan -&gt;
implement workflow where implementation cannot begin until the plan is
approved. The plan approval is the boundary. Superpowers&#39; two-stage
review (spec compliance, then code quality) provides a quality gate, but
both reviews are performed by subagents of the same system — no holdout
separation.</p>
<p><strong>GitHub&#39;s Spec Kit</strong> structures the process as Specify
-&gt; Plan -&gt; Tasks -&gt; Implement, with explicit checkpoints. The
human must validate each phase before advancing. But the implementation
agent has access to the specification (including any embedded examples
or test descriptions), creating the reward-hacking vulnerability that
holdout scenarios address.</p>
<p><strong>Anthropic&#39;s multi-agent research system [<a href="#ref-18">18</a>]</strong> demonstrates the orchestrator-worker
pattern at production scale — a lead agent coordinates specialized
subagents with clear task boundaries and output formats. The
architecture is directly applicable to factory orchestration, with the
lead agent as the Attractor&#39;s graph traversal engine and subagents
executing individual phases.</p>
<p>None of these systems fully solves the interactive/non-interactive
boundary. The solution likely emerges from combining StrongDM&#39;s shift
work concept (as framing), Superpowers&#39; mandatory workflow gates (as
interactive-phase structure), holdout scenario separation (as
non-interactive-phase integrity), and Anthropic&#39;s orchestrator-worker
pattern (as execution architecture).</p>
<hr />
<h2 id="8-evolving-specifications-over-time">8. Evolving Specifications
Over Time</h2>
<p>Specifications are living documents. As the service operates in
production, encounters new usage patterns, and integrates with evolving
SOA neighbors, the specifications must evolve.</p>
<h3 id="specification-versioning">Specification Versioning</h3>
<p>Specifications live in version control alongside (but structurally
separate from) generated code. Every change should be a deliberate,
reviewed commit with a clear rationale. This is where the context/intent
engineer spends most of their ongoing time.</p>
<p>A specification change may cascade. Amending intent may invalidate
existing scenarios, require new ones, or necessitate constraint updates.
Changing a contract (because a neighboring service updated its API) may
require intent changes to handle new capabilities or removed features.
The specification engineer must trace these cascades manually, though
agents can assist by identifying potential inconsistencies between
updated and not-yet-updated specification files.</p>
<h3 id="patterns-from-existing-systems">Patterns from Existing
Systems</h3>
<p><strong>8090&#39;s drift detection</strong> (in their Foundry module)
continuously analyzes changes in requirements and codebase to detect
drift from existing blueprints. When code changes, PRDs and engineering
plans are automatically synchronized. This bidirectional synchronization
is the closest existing implementation to automated specification
co-evolution, though 8090 focuses on the requirements &lt;-&gt; code
relationship rather than specification &lt;-&gt; production.</p>
<p><strong>GitHub Spec Kit&#39;s bidirectional feedback</strong> principle:
&quot;Production reality informs specification evolution. Metrics, incidents,
and operational learnings become inputs for specification refinement.&quot;
This is aspirational — the toolkit provides specification structure but
not the production-feedback mechanism. The principle is right; the
tooling hasn&#39;t caught up.</p>
<p><strong>Superpowers&#39; memory extraction</strong> demonstrates a
different form of specification evolution. Jesse Vincent extracted 2,249
markdown files of lessons learned from previous Claude Code
conversations — corrections, issues, pattern discoveries — and
incorporated them into the skills framework. This is specification
evolution through operational learning: the system gets smarter by
mining its own history. The factory analogue: mining CXDB&#39;s convergence
logs to identify patterns that should become specification amendments or
new constraints.</p>
<p><strong>StrongDM&#39;s gene transfusion</strong> moves working patterns
between codebases by pointing agents at concrete exemplars. For
specification evolution, this means a successful pattern in one
service&#39;s specification can be &quot;transfused&quot; into another — not by
copying code, but by copying the specification patterns that led to
successful implementations.</p>
<h3 id="the-spec-amendment-workflow">The Spec Amendment Workflow</h3>
<p>When a specification needs to change (triggered by production signal,
new requirements, or contract evolution), the process returns to the
interactive shift. The specification engineer identifies the change
needed, often informed by production signals (<a href="#6-production-observability-and-specification-evolution">Section
6</a>) or evolving business requirements. Spec-refinement agents assist
in drafting the amendment, checking consistency with existing
specifications, identifying cascade effects. New holdout scenarios are
authored (or existing ones updated) to validate the changed behavior.
The specification change is committed and the factory re-runs. The
non-interactive shift produces a new build. Holdout validation verifies
the new build satisfies both changed scenarios and all existing ones
(regression protection).</p>
<p>This cycle — observe, amend, rebuild, validate — is the heartbeat of
an evolving factory-built service. Its cadence depends on the service&#39;s
rate of change and the organization&#39;s tolerance for specification
drift.</p>
<hr />
<h2 id="9-soa-boundary-coordination">9. SOA Boundary Coordination</h2>
<p>This guide focuses on building a single service. But that service
exists within an SOA, and its contracts connect it to neighboring
services.</p>
<h3 id="the-coordination-problem">The Coordination Problem</h3>
<p>When your service&#39;s downstream contract changes (because the
specification evolved), every consuming service must adapt. When an
upstream service changes its contract, your specification must be
updated.</p>
<p>In a traditional SOA, contract changes are coordinated through
versioning, deprecation cycles, and cross-team communication. In a
factory-built service, the challenge is that contract changes may
cascade quickly — if the factory can rebuild a service in hours, the
rate of potential contract change increases dramatically.</p>
<p>The interesting, unsolved question: can contract change propagation
itself be automated? If Service A&#39;s factory updates its downstream
contract, could that change automatically propagate to Service B&#39;s
upstream contract, triggering Service B&#39;s factory to rebuild? A &quot;factory
of factories&quot; — SOA-level orchestration coordinating specification
evolution across service boundaries.</p>
<h3 id="out-of-scope">Out of Scope</h3>
<p>This guide does not provide implementation guidance for cross-service
contract propagation. The problem compounds single-service factory
complexity with SOA governance challenges. Start with a single service,
gain confidence in the specification -&gt; factory -&gt; validation
loop, then consider cross-service automation.</p>
<p>The <code>spec/contracts/</code> directory captures contracts as they
exist today. When upstream services change, the specification engineer
updates the contract files manually and triggers a factory rebuild. This
manual coordination is a pragmatic starting point, not an architectural
limitation. As tooling for automated contract propagation develops, the
contracts directory is the integration point.</p>
<hr />
<h2 id="10-applicability-beyond-soa">10. Applicability Beyond SOA</h2>
<p>While this guide targets services within an enterprise SOA, the
factory pattern applies more broadly. The core mechanics — specification
as primary artifact, holdout scenario validation, graph-structured agent
orchestration, satisfaction-based convergence — are
domain-independent.</p>
<p><strong>Libraries and frameworks</strong> can be factory-built by
replacing API contracts with public interface specifications and using
consumer codebases (or synthetic consumers) for validation. Holdout
scenarios become usage examples the library must satisfy.</p>
<p><strong>Standalone applications</strong> replace SOA contracts with
user interface specifications and interaction scenarios. The DTU concept
adapts to simulating user interactions rather than service-to-service
calls.</p>
<p><strong>Infrastructure and configuration</strong> (Terraform modules,
Kubernetes manifests, CI/CD pipelines) can be factory-built by
specifying desired state as intent, cloud provider APIs as contracts,
and compliance requirements as constraints. Validation runs generated
infrastructure in a sandbox against scenarios verifying correct
resources with correct configuration.</p>
<p>The common thread: humans specify intent and validation criteria, the
factory produces implementation, a separate validation process verifies
correctness without inspecting internals. This pattern works wherever
the &quot;what&quot; can be expressed independently of the &quot;how.&quot;</p>
<hr />
<h2 id="11-toolchain-summary">11. Toolchain Summary</h2>
<p>This section maps the abstract concepts to concrete tools. The
landscape evolves rapidly; treat this as a snapshot of early 2026 rather
than a permanent recommendation.</p>
<h3 id="specification-authoring">Specification Authoring</h3>
<p>Specifications are markdown files authored in any text editor. For
agent-assisted refinement during the interactive shift, any agentic TUI
that supports interactive conversation will work — Claude Code,
OpenCode, Cursor, and Copilot Workspace all support specification-first
workflows. GitHub&#39;s Spec Kit provides scaffolding (constitution,
specification, plan, and task templates) that can bootstrap the
<code>spec/</code> directory structure. BMAD&#39;s planning agents provide a
more opinionated interactive workflow for teams that prefer structured
guidance during specification authoring. The SPARC methodology [<a href="#ref-19">19</a>] takes a similar phased approach — Specification,
Pseudocode, Architecture, Refinement, Completion — implemented as a
multi-agent orchestration platform built on claude-flow.</p>
<p>No third-party tooling is required. Direct prompts in your TUI of
choice are sufficient — describe the specification, ask the agent to
identify gaps, refine iteratively. For teams wanting more structure,
custom skills can be built from scratch. A useful pattern: a
Critique/Revise loop. A <code>critique</code> skill feeds the current
specification to multiple LLMs and aggregates their critiques. A paired
<code>revise</code> skill presents each critique to a revision agent,
which either accepts the change or rejects it with a written rationale,
recording outcomes in an acknowledgement file. Running this loop in a
versioned way — committing each iteration — produces an auditable
specification evolution history, capturing which changes originated from
human judgment and which from LLM suggestion.</p>
<h3 id="factory-orchestration">Factory Orchestration</h3>
<p>StrongDM&#39;s Attractor is the reference implementation of
graph-structured, non-interactive factory orchestration. Its NLSpec is
published on GitHub and can be implemented using any coding agent as the
execution substrate. Kilroy (a Go implementation of the Attractor
pattern) provides DOT-based phase graph execution with convergence
tracking. The choice of orchestration layer matters less than the
properties it must satisfy: graph-structured execution, provider-aligned
agent tooling, convergence detection, and integration with the
validation harness.</p>
<h3 id="context-and-state">Context and State</h3>
<p>CXDB (StrongDM&#39;s context database) provides an immutable DAG for
storing agent conversation histories and tool outputs. Simpler
alternatives for teams not ready to deploy CXDB: structured logging of
agent sessions to a queryable store, persistent task/state systems built
into modern coding agents, or file-based context following StrongDM&#39;s
&quot;filesystem as memory&quot; technique.</p>
<h3 id="agent-identity">Agent Identity</h3>
<p>StrongDM ID provides identity management for humans, workloads, and
AI agents with federated authentication. The key requirement for teams
using existing infrastructure: factory agents should operate with scoped
credentials — access only to the resources their phase requires, not
blanket access to organizational systems.</p>
<h3 id="skills-and-capabilities">Skills and Capabilities</h3>
<p>The skills framework (exemplified by Superpowers, BMAD&#39;s
agent-as-code definitions, and native skills/plugins in modern agentic
TUIs) provides the abstraction layer between the orchestration graph and
actual tools. Skills wrap CLIs, MCP servers, APIs, and deterministic
tools into capabilities that agents invoke during phase execution. The
specific implementations matter less than the abstraction: the factory
graph references capabilities, not specific tool invocations.</p>
<h3 id="agentic-tui">Agentic TUI</h3>
<p>The agentic TUI is the human&#39;s primary interface to the factory — the
tool through which the context/intent engineer collaborates with
spec-refinement agents, kicks off factory runs, and monitors
convergence. The specific choice (Claude Code, OpenCode, Cursor, or
equivalent) matters less than the required properties: interactive
conversation, subagent orchestration, MCP integration, and a
skills/plugin ecosystem. The TUI is a means, not an end.</p>
<hr />
<h2 id="12-getting-started">12. Getting Started</h2>
<h3 id="start-experimenting">Start experimenting</h3>
<p>The best way to learn factory-pattern development is to try it. The
concepts in this guide are easier to internalize once you have run a
factory loop end-to-end, even on a trivial service. Start small — a
single-endpoint service with a handful of scenarios — and let the
experience shape your understanding of where specifications need
precision, where holdout scenarios catch genuine issues, and where the
factory diverges from intent in ways that reveal specification gaps. No
prescribed sequence can substitute for hands-on calibration.</p>
<h3 id="let-your-agents-read-this-guide">Let your agents read this
guide</h3>
<p>There is a markdown version of this paper available at <a href="https://github.com/thewoolleyman/software-factory-practitioners-guide"><span>https://github.com/thewoolleyman/software-factory-practitioners-guide</span></a>.
You can download it and feed parts of it to your agents as context for
them to understand how software factories work. It is licensed under the
<a href="https://unlicense.org/">UNLICENSE</a>. You may modify, update,
or use it in any way that is useful to you and your agents.</p>
<h3 id="notes-on-usage-of-kilroy">Notes on usage of Kilroy</h3>
<p>Most of the practical experience behind this guide came from a forked
version of Kilroy [<a href="#ref-8">8</a>], with several bug fixes
applied (PRs submitted upstream). Kilroy was chosen because it was
relatively feature-complete compared to other Attractor implementations
and because it is written in Go — straightforward to patch and evolve
when bugs or edge cases surfaced during real factory runs.</p>
<p>One necessary change: making the Graphviz DOT pipeline generation
deterministic. By default, Kilroy uses LLMs to generate the graphs,
which produced non-deterministic output when given the raw consolidated
specification as input.</p>
<p>A configuration system was created to remove the LLM from pipeline
generation. This enforces deterministic, repeatable, idempotent
generation for the Graphviz pipeline DOT file. The configuration lives
in <code>factory/attractor_config</code> and consists of markdown files
plus a YAML DSL to represent graph structure. Everything eventually ends
up as YAML DSL.</p>
<p>This is wired together with Claude Code skills in a
<code>kilroy:</code> namespace. The logic supporting the configuration
system lives in the skills themselves, with tests. Example skill
structure:</p>
<pre class="text"><code>.claude
└── skills
    ├── cxdb-status
    │  └── SKILL.md
    ├── kilroy-generate-pipeline
    │  ├── README.md
    │  ├── SKILL.md
    │  ├── script
    │  │  ├── compile_dot.rb
    │  │  ├── extract_field.rb
    │  │  ├── extract_prompts.rb
    │  │  ├── generate_pipeline.rb
    │  │  ├── patch_dot.rb
    │  │  ├── render_prompt.rb
    │  │  └── verify_dot.rb
    │  └── tests
    │      ├── run_tests.sh
    │      ├── test_compile_dot.rb
    │      ├── test_extract_field.rb
    │      ├── test_extract_prompts.rb
    │      ├── test_generate_pipeline.rb
    │      ├── test_patch_dot.rb
    │      ├── test_render_prompt.rb
    │      └── test_verify_dot.rb
    ├── kilroy-help
    │  └── SKILL.md
    ├── kilroy-land
    │  └── SKILL.md
    ├── kilroy-run
    │  └── SKILL.md
    ├── kilroy-setup
    │  └── SKILL.md
    ├── kilroy-status
    │  └── SKILL.md</code></pre>
<p>These are written in Ruby with fully self-contained, zero-dependency
<code>minitest</code> tests. Any language works, as long as the scripts
are executable from within skills.</p>
<p>The YAML DSL and support skills may eventually be released as open
source, likely as a Claude Code plugin. But as long as you understand
the <em>intent</em> — a wrapper around Kilroy&#39;s UX with deterministic
pipeline generation — you can implement the same pattern yourself in any
language. Bash is a great option; LLMs write solid Bash code and tests.
This is also a good exercise to build your Level 4 skills in Shapiro&#39;s
taxonomy [<a href="#ref-1">1</a>].</p>
<hr />
<h2 id="13-out-of-scope">13. Out of Scope</h2>
<p>This guide focuses on the factory mechanics: turning human intent
into working, deployed software for a single service. Several important
concerns are acknowledged here as real, critical, and required for
enterprise adoption — but separate from the core factory pattern.</p>
<p><strong>Governance, auditability, and compliance.</strong> Who
approves specification changes? What audit trail is required for
factory-produced code? How do you satisfy SOC 2, HIPAA, or other
compliance frameworks when no human reviews the code? The Stanford Law
School&#39;s analysis [<a href="#ref-4">4</a>], published two days after
StrongDM&#39;s announcement, frames this sharply: &quot;When a customer asks &#39;how
was this software built?&#39; the truthful answer is: &#39;Coding agents wrote
it. Other agents tested it against replicas of your services.&#39;&quot; Existing
software liability frameworks assume human involvement in code
production. Factory-built software challenges that assumption. This
guide does not address how to navigate it.</p>
<p><strong>Agent security and scope.</strong> Factory agents operate
with access to source code, build systems, and potentially production
infrastructure. What permissions should they have? How do you prevent a
misbehaving agent from exfiltrating secrets, corrupting data, or
deploying a bad build? Agent sandboxing, credential scoping, and blast
radius limitation are critical. StrongDM ID&#39;s path-scoped sharing model
is one approach, but agent security architecture deserves its own
treatment.</p>
<p><strong>Organizational transformation.</strong> Adopting the factory
pattern changes what it means to be a software engineer. The skill
profile shifts from code production to specification authoring, scenario
design, and system thinking. The Stanford Law analysis notes: &quot;The skill
of reading and writing code, the bedrock of software engineering for
seventy years, becomes unnecessary&quot; in a full dark factory. How
organizations manage this transition — retraining, role redefinition,
hiring, career ladders — is a significant challenge this guide does not
address.</p>
<p><strong>Cost management.</strong> StrongDM&#39;s $1,000/day benchmark is
aggressive. Not every organization can or should operate at that spend
level. Optimizing token costs, choosing between expensive reasoning
models and cheaper fast models, and projecting factory operating costs
are left to you.</p>
<hr />
<h2 id="14-open-questions">14. Open Questions</h2>
<p>The factory pattern is new enough that fundamental questions remain
open. These have been referenced inline and are consolidated here.</p>
<h3 id="specification-completeness-and-validation">Specification
Completeness and Validation</h3>
<p><strong>When is a specification &quot;complete enough&quot; for non-interactive
execution?</strong> StrongDM&#39;s shift work technique provides framing,
but no formal criterion exists. The new-hire test and scenario coverage
heuristics help, but completeness remains a judgment call. Formal
methods — perhaps adapted from model checking or type theory — could
eventually provide stronger guarantees.</p>
<p><strong>What is the right satisfaction threshold?</strong>
Probabilistic validation requires choosing a threshold. Is 95%
acceptable? 99%? Does it vary by category (higher for security-critical
paths, lower for cosmetic behavior)? The answer is domain-specific, but
no established framework exists for making these decisions
systematically.</p>
<p><strong>How should scenario authoring scale?</strong> For a service
with hundreds of behavioral paths, hand-authoring holdout scenarios
bottlenecks on human throughput. Agent-assisted authoring using separate
agent sets is emerging, but the mechanics — ensuring quality, avoiding
redundancy, maintaining coverage as specifications evolve — are still
being worked out.</p>
<h3 id="production-signals-and-specification-evolution">Production
Signals and Specification Evolution</h3>
<p><strong>How do production signals become specification
amendments?</strong> <a href="#6-production-observability-and-specification-evolution">Section
6</a> sketched three signal categories and a triage layer, but no
existing system implements this end-to-end. The gap between &quot;SRE detects
anomaly&quot; and &quot;specification engineer amends intent&quot; is bridged by human
judgment. Whether and how that gap can be narrowed through automation is
the most important open research question for factory-pattern
systems.</p>
<h3 id="scaling-and-complexity">Scaling and Complexity</h3>
<p><strong>Can factory patterns work for large, complex
services?</strong> StrongDM&#39;s published examples involve a three-person
team building services over a few months. How the pattern scales to
services with thousands of behavioral paths, deep dependency chains, and
years of accumulated domain complexity is unproven. The specification
engineering challenge alone — capturing all that complexity in NLSpec —
may prove to be a bottleneck that resists automation.</p>
<p><strong>How does SOA boundary coordination scale?</strong> As
discussed in <a href="#9-soa-boundary-coordination">Section 9</a>,
propagating contract changes across multiple factory-built services is
unsolved. Organizations with dozens or hundreds of services need
governance and automation beyond this guide&#39;s single-service scope.</p>
<p><strong>Can factory patterns apply to monoliths and
monorepos?</strong> These present fundamentally different challenges. A
well-structured monolith is excellent for <em>humans</em> managing
context without distributed-system complexity — all code in one place,
indexable and navigable in an IDE. With agentic engineering, the
constraints are different. Agents have no pre-existing codebase
knowledge outside what they are given. Context can be stored and managed
externally, but even the largest models of today only have roughly a
million tokens of context. That is infinitesimally small compared to the
domain knowledge a human expert carries. These patterns are currently
far more tractable in smaller, loosely-coupled, highly-cohesive service
repos.</p>
<h3 id="enterprise-integration-and-auditability">Enterprise Integration
and Auditability</h3>
<p><strong>How does the factory interact with existing CI/CD?</strong>
The factory produces build artifacts. How those flow through enterprise
CI/CD pipelines — testing, staging, approval, deployment — is an
integration challenge for each organization&#39;s infrastructure team. The
factory&#39;s output should be a deployable artifact; what happens afterward
depends on deployment practices.</p>
<p><strong>How do specifications relate to existing SDLC
tooling?</strong> Most enterprise teams manage development through
external systems — epics and issues in GitLab, product roadmaps, design
documents across the SDLC toolchain. The factory pattern places
specifications in the repository as the authoritative build input. But
those specifications describe the same intent an epic or issue set would
describe. Which is the source of truth? If a product manager updates an
epic, does that flow into <code>spec/intent/</code>? If the
specification engineer amends intent based on production signals, does
that propagate back to the tracker?</p>
<p>This matters because these external systems are not optional — they
are where cross-functional teams coordinate, where compliance processes
attach, where audit trails live, where leadership tracks progress. A
factory that ignores existing SDLC tooling creates a parallel process
competing with the established workflow. How to bridge specification
files with artifacts in external systems — and which direction
information flows when they diverge — is an integration design problem
each organization must solve.</p>
<p><strong>How do enterprises maintain code provenance and supply chain
auditability?</strong> Factory code lacks the commit history and PR
trails that conventional audit workflows assume — but the gap is
architectural, not fatal. Treat specifications as the auditable source
and generated code as a build artifact, like compiled binaries. The
provenance chain becomes: spec commit → factory run → artifact.
Specification changes serve as authorized requests, factory runs as
documented builds, holdout validation — where reserved tests verify the
output independently — as approval. This maps to the intent behind SOC
2, SLSA, and similar frameworks, which require authorization,
documentation, testing, and approval of changes but do not prescribe
commit granularity. The open question is whether auditors will accept
specification-level provenance or insist on code-level review that the
factory pattern deliberately eliminates.</p>
<p><strong>Do new paradigms for source control management need to
emerge?</strong> Git records what changed. Factories also need to record
how and why — which phases ran, which spec sections drove changes, where
convergence loops retried. The dominant industry response so far is
provenance tooling layered on top of Git: attribution metadata in Git
Notes, agent trace formats that capture prompts and reasoning alongside
diffs, and provenance bills of materials linking artifacts to their
specifications. Jujutsu (jj) [<a href="#ref-20">20</a>] offers a
complementary approach — its operation log captures every repository
mutation in an undo-capable history that survives squash-commits,
preserving spec-to-code links through iterative refinement, while using
Git as a storage backend so organizations can experiment without
migration. These are promising affordances, not proven solutions — but
the need is clear: factory-built software needs process provenance as a
first-class concern, whether through new VCS primitives or provenance
layers atop existing tools.</p>
<h3 id="generated-code-as-artifact">Generated Code as Artifact</h3>
<p><strong>Should generated code be checked into version
control?</strong> This guide recommends it (auditability, rollback,
CI/CD integration). Sandgarden&#39;s alternative [<a href="#ref-10">10</a>]
treats source code as ephemeral output, never committed — contributions
happen through specifications, not code. The tradeoffs between these
models deserve ongoing evaluation as the pattern matures and tooling
evolves.</p>
<hr />
<h2 id="15-closing-perspective">15. Closing Perspective</h2>
<h3 id="whats-next">What&#39;s next?</h3>
<p>Everything in this guide is a snapshot of a field changing at
extraordinary pace. The tools, frameworks, and implementations
referenced here represent what was available as of early 2026. Many will
be superseded, forked, rewritten, or abandoned within months. New
entrants will appear addressing the same goals with different
architectures and tradeoffs. Normal for a domain this young.</p>
<p>Kilroy was chosen for the practical work behind this guide because it
was relatively complete and easy to patch when it wasn&#39;t. It does not
have to be used. The Attractor itself is a specification — an NLSpec
published on GitHub describing what a non-interactive coding agent
should do, not how any particular implementation must do it. Anyone can
implement their own factory orchestration from that specification, adapt
it, or build something better. The specification is the contribution;
the implementation is replaceable. Fittingly, that is the same principle
the factory pattern applies to software itself.</p>
<p>The out-of-scope items in <a href="#13-out-of-scope">Section 13</a> —
governance, agent security, organizational transformation, cost
management — and the open questions in <a href="#14-open-questions">Section 14</a> are not minor footnotes. They
are hard problems that will determine whether the factory pattern moves
from early-adopter experimentation to mainstream enterprise practice.
This guide does not pretend to have answers. It offers a framework for
thinking about them, grounded in the published experience of teams that
have built real software this way.</p>
<p>The deeper point is not about any particular tool or technique. The
trajectory of software development is moving toward
specification-driven, agent-executed production. The economics are
compelling: if a specification can drive agents to produce correct,
validated software faster and cheaper than human implementation, the
industry will adopt it — unevenly, reluctantly in some quarters, but
inevitably. The skill that matters is not mastering today&#39;s toolchain.
It is learning to think in specifications, to express intent precisely
enough that machines can act on it, and to design validation systems
rigorous enough to trust the output. Those skills transfer across
whatever tools come next.</p>
<hr />
<h2 id="references-and-further-reading">References and Further
Reading</h2>
<p><a id="ref-1"></a>[1] <strong>Dan Shapiro, &quot;The Five Levels: from
Spicy Autocomplete to the Dark Factory.&quot;</strong> <a href="https://www.danshapiro.com/blog/2026/01/the-five-levels-from-spicy-autocomplete-to-the-software-factory/">danshapiro.com/blog/2026/01/the-five-levels-from-spicy-autocomplete-to-the-software-factory</a>,
January 2026 — The five-level taxonomy of AI-assisted programming.</p>
<p><a id="ref-2"></a>[2] <strong>StrongDM Attractor NLSpec.</strong> <a href="https://github.com/strongdm/attractor">github.com/strongdm/attractor</a>
— The open specification for a non-interactive coding agent structured
as a graph of phases.</p>
<p><a id="ref-3"></a>[3] <strong>StrongDM Software Factory.</strong> <a href="https://factory.strongdm.ai">factory.strongdm.ai</a> — The primary
reference for factory-pattern development, including principles,
techniques (shift work, gene transfusion, DTU, pyramid summaries), and
products (Attractor, CXDB, StrongDM ID). The shift work technique is
documented at <a href="https://factory.strongdm.ai/techniques/shift-work">factory.strongdm.ai/techniques/shift-work</a>.</p>
<p><a id="ref-4"></a>[4] <strong>Stanford Law School CodeX, &quot;Built by
Agents, Tested by Agents, Trusted by Whom?&quot;</strong> <a href="https://law.stanford.edu/2026/02/08/built-by-agents-tested-by-agents-trusted-by-whom/">law.stanford.edu/2026/02/08/built-by-agents-tested-by-agents-trusted-by-whom</a>,
February 2026 — Legal and accountability analysis of factory-produced
software.</p>
<p><a id="ref-5"></a>[5] <strong>8090 Software Factory.</strong> <a href="https://www.8090.ai/docs/general/introduction">8090.ai/docs/general/introduction</a>
— AI-native SDLC platform with Refinery (requirements), Foundry
(blueprints), Planner (work orders), and Validator (feedback loop)
modules.</p>
<p><a id="ref-6"></a>[6] <strong>Jesse Vincent, Superpowers.</strong> <a href="https://github.com/obra/superpowers">github.com/obra/superpowers</a>
— Agentic skills framework implementing mandatory brainstorm -&gt; plan
-&gt; implement workflow with DOT graph experimentation.</p>
<p><a id="ref-7"></a>[7] <strong>GitHub Spec Kit.</strong> <a href="https://github.com/github/spec-kit">github.com/github/spec-kit</a>
— Open-source toolkit for specification-driven development, including
the Spec-Driven Development methodology document.</p>
<p><a id="ref-8"></a>[8] <strong>Kilroy (forked).</strong> <a href="https://github.com/thewoolleyman/kilroy">github.com/thewoolleyman/kilroy</a>
— A fork of Kilroy (a Go implementation of the Attractor pattern) with
bug fixes applied. PRs for these fixes have been submitted upstream.
Kilroy was selected for its relative feature completeness among
Attractor implementations and the accessibility of Go for patching and
evolution.</p>
<p><a id="ref-9"></a>[9] <strong>Anthropic, &quot;2026 Agentic Coding Trends
Report.&quot;</strong> <a href="https://resources.anthropic.com/hubfs/2026%20Agentic%20Coding%20Trends%20Report.pdf">resources.anthropic.com/hubfs/2026
Agentic Coding Trends Report.pdf</a> — Industry data on AI-assisted
development adoption and multi-agent coordination patterns.</p>
<p><a id="ref-10"></a>[10] <strong>Sandgarden (sgai).</strong> <a href="https://github.com/sandgardenhq/sgai">github.com/sandgardenhq/sgai</a>
— Goal-based development framework where specifications are the
contribution artifact and source code is treated as generated
output.</p>
<p><a id="ref-11"></a>[11] <strong>Birgitta Böckeler / ThoughtWorks,
&quot;Understanding Spec-Driven-Development: Kiro, spec-kit, and
Tessl.&quot;</strong> <a href="https://martinfowler.com/articles/exploring-gen-ai/sdd-3-tools.html">martinfowler.com/articles/exploring-gen-ai/sdd-3-tools.html</a>
— Critical comparison of SDD tools with honest assessment of
limitations.</p>
<p><a id="ref-12"></a>[12] <strong>Simon Willison, &quot;How StrongDM&#39;s AI
team build serious software without even looking at the code.&quot;</strong>
<a href="https://simonwillison.net/2026/Feb/7/software-factory/">simonwillison.net/2026/Feb/7/software-factory/</a>,
February 2026 — Detailed first-person account of visiting StrongDM&#39;s
factory team.</p>
<p><a id="ref-13"></a>[13] <strong>Cem Kaner, &quot;An Introduction to
Scenario Testing.&quot;</strong> 2003 — The foundational work on
scenario-based testing that inspired StrongDM&#39;s holdout approach.</p>
<p><a id="ref-14"></a>[14] <strong>Microsoft, &quot;An AI led SDLC: Building
an End-to-End Agentic Software Development Lifecycle with Azure and
GitHub.&quot;</strong> <a href="https://techcommunity.microsoft.com/blog/appsonazureblog/an-ai-led-sdlc-building-an-end-to-end-agentic-software-development-lifecycle-wit/4491896">techcommunity.microsoft.com/blog/appsonazureblog/an-ai-led-sdlc-building-an-end-to-end-agentic-software-development-lifecycle-wit/4491896</a>,
February 2026 — Practical demonstration of spec-driven development with
coding agents, quality reviews, and Azure SRE Agent for production
monitoring.</p>
<p><a id="ref-15"></a>[15] <strong>Stack Overflow Blog, &quot;How
observability-driven development creates elite performers.&quot;</strong> <a href="https://stackoverflow.blog/2022/10/12/how-observability-driven-development-creates-elite-performers/">stackoverflow.blog/2022/10/12/how-observability-driven-development-creates-elite-performers</a>
— Process capability framework connecting observability to specification
limits.</p>
<p><a id="ref-16"></a>[16] <strong>BMAD Method.</strong> <a href="https://github.com/bmad-code-org/BMAD-METHOD">github.com/bmad-code-org/BMAD-METHOD</a>
— Agile AI-driven development framework with specialized agent personas
and scale-adaptive planning.</p>
<p><a id="ref-17"></a>[17] <strong>Factory.</strong> <a href="https://factory.ai">factory.ai</a> — Agent-native software
development platform that delegates development tasks to autonomous AI
agents (&quot;Droids&quot;) within existing tools and workflows.</p>
<p><a id="ref-18"></a>[18] <strong>Anthropic, &quot;How we built our
multi-agent research system.&quot;</strong> <a href="https://www.anthropic.com/engineering/multi-agent-research-system">anthropic.com/engineering/multi-agent-research-system</a>
— Production architecture for orchestrator-worker multi-agent
systems.</p>
<p><a id="ref-19"></a>[19] <strong>SPARC Methodology /
claude-flow.</strong> <a href="https://github.com/ruvnet/claude-flow">github.com/ruvnet/claude-flow</a>
— Multi-agent orchestration platform with Specification -&gt; Pseudocode
-&gt; Architecture -&gt; Refinement -&gt; Completion phased
development.</p>
<p><a id="ref-20"></a>[20] <strong>Jujutsu (jj).</strong> <a href="https://www.jj-vcs.dev/latest/">jj-vcs.dev</a> — A Git-compatible
version control system with first-class conflicts, operation-log undo,
automatic rebasing, and working-copy-as-commit semantics. <a href="https://github.com/jj-vcs/jj">github.com/jj-vcs/jj</a></p>
<p><a id="ref-21"></a>[21] <strong>Robert M. Pirsig, <em>Zen and the Art
of Motorcycle Maintenance</em>.</strong> <a href="https://en.wikipedia.org/wiki/Zen_and_the_Art_of_Motorcycle_Maintenance">en.wikipedia.org/wiki/Zen_and_the_Art_of_Motorcycle_Maintenance</a>
— A philosophical exploration of the concept of &quot;quality&quot; and the
tension between romantic and classical approaches to understanding.</p>
<hr />
<h2 id="appendix-a-personal-note">Appendix: A Personal Note</h2>
<p>I want to close on a personal note. <strong>I&#39;ve been writing code
professionally for 35 years, 40 years in total. I&#39;ve written and
committed, by a rough estimate, somewhere around 500 thousand lines of
code</strong> (even before AI, there were code generators, so estimates
are highly subjective). That is with a single brain (also highly
subjective, I know), two hands, and ten fingers. And sometimes with
another human - pair-programmed code probably counts for about a quarter
of that (I pair-programmed mostly full-time in the 13 years I worked at
Pivotal Labs).</p>
<p>I pride myself on being a disciplined craftsperson. One who finds joy
and passion in my craft. One who crafts code with love and care for the
other humans who will see it, improve it, use it, and benefit from
it.</p>
<p><strong>In the last 2-3 months since I started agentic coding, I have
committed roughly 150,000 lines of code. I wrote almost none of it by
hand, probably fewer than 500 lines.</strong> This is across multiple
working, usable projects. Mostly personal greenfield projects, and also
some internal bug/feature work at my company. It was mostly Level 3-4
work in Shapiro&#39;s taxonomy [<a href="#ref-1">1</a>].</p>
<p>And as noted in the first sections of this paper, I want to be fully
transparent: <strong><em>I haven&#39;t yet produced any usable work from a
fully-Level-5 &quot;Dark Software Factory&quot; approach, in the week or two I
have been experimenting</em></strong>. I have learned that to be
successful with this approach, you must learn a new and very difficult
skill: writing specifications with a level of rigor and precision that
allows a factory to produce usable, quality code.</p>
<p>It&#39;s outside the scope of this paper to discuss the <em>quality</em>
of the code I directed AI to produce with my Level 3-4 work, or that a
Level 5 fully-automated factory will produce, even when it is provided
the best specifications and validations. Everyone will have a different
perspective on quality [<a href="#ref-21">21</a>].</p>
<p>What I <em>can</em> say is that the working Level 3-4 code I have
produced is undoubtedly of a <em>different</em> quality than the code I
have written by hand over the last 40 years. But is it better? Worse?
Both, depending on how you look at it? Yes, all of the above. Again,
highly subjective.</p>
<p>What I do know is this: software development will never be the same.
I&#39;m more convinced of that than anything in my last 40 years of doing
it. Will it change overnight? No, of course not. Will it change almost
everywhere, eventually? Yes.</p>
</body>
</html>
